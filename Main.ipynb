{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNerN/xx/EpY8e/27zTE1dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werd0n4/hand-gesture-classification/blob/master/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWV-fQOYLdCT"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q8kzcXXwFg7",
        "outputId": "bf799303-1b0b-4be4-d202-aeea4fb91225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XymGlouwU2-"
      },
      "source": [
        "# !ls"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBEyz2mvZcZe"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ljCHYlFLKus"
      },
      "source": [
        "# Constant parameters\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgEIncCL4j8i"
      },
      "source": [
        "horiz_x = 800  \n",
        "vert_y = 600\n",
        "#drive paths\n",
        "#\n",
        "imgs_train_path = '/content/drive/My Drive/Colab Notebooks/Rozszerzony_dataset/Train/'\n",
        "imgs_test_path = '/content/drive/My Drive/Colab Notebooks/Rozszerzony_dataset/Test/'\n",
        "#local paths windows\n",
        "#\n",
        "# imgs_train_path = 'C:\\\\Users\\\\Werdon\\\\Google Drive\\\\Colab Notebooks\\\\Rozszerzony_dataset\\\\Train'\n",
        "# imgs_test_path = 'C:\\\\Users\\\\Werdon\\\\Google Drive\\\\Colab Notebooks\\\\Rozszerzony_dataset\\\\Train'\n",
        "#local paths manjaro\n",
        "#\n",
        "# imgs_train_path = '/home/werdon4/Rozszerzony_dataset/Train'\n",
        "# imgs_test_path = '/home/werdon4/Rozszerzony_dataset/Test'\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4fPPXAU7hhc"
      },
      "source": [
        "\n",
        "class_names = {\n",
        "    0: \"1\",\n",
        "    1: \"2\",\n",
        "    2: \"3\",\n",
        "    3: \"4\",\n",
        "    4: \"5\",\n",
        "    5: \"A\",\n",
        "    6: \"B\",\n",
        "    7: \"C\",\n",
        "    8: \"D\",\n",
        "    9: \"E\",\n",
        "    10: \"F\",\n",
        "    11: \"G\",\n",
        "    12: \"H\",\n",
        "    13: \"I\",\n",
        "    14: \"K\",\n",
        "    15: \"L\",\n",
        "    16: \"M\",\n",
        "    17: \"N\",\n",
        "    18: \"O\",\n",
        "    19: \"P\",\n",
        "    20: \"R\",\n",
        "    21: \"S\",\n",
        "    22: \"T\",\n",
        "    23: \"U\",\n",
        "    24: \"W\",\n",
        "    25: \"Y\",\n",
        "    26: \"Z\"\n",
        "}\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixFACs6TLptD"
      },
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wS1Y3LhdoCk"
      },
      "source": [
        "## Image resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WA7zKkLxRZk"
      },
      "source": [
        "\n",
        "def resize(path):\n",
        "    img_counter = 0\n",
        "\n",
        "    for dirname in os.listdir(path): \n",
        "        for filename in os.listdir(os.path.join(path, dirname)):\n",
        "            image_path = os.path.join(path, dirname, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "            resized_img = cv2.resize(img, (horiz_x, vert_y))\n",
        "            cv2.imwrite(image_path, resized_img)\n",
        "            img_counter += 1\n",
        "    \n",
        "    print('Images in set: ' + str(img_counter))\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlo-plFYdu3g"
      },
      "source": [
        "## Image size sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_RAtl1TrwJL"
      },
      "source": [
        "\n",
        "def sanity_check(path):\n",
        "    counter = 0\n",
        "\n",
        "    for dirname in os.listdir(path): \n",
        "        for filename in os.listdir(os.path.join(path, dirname)):\n",
        "            image_path = os.path.join(path, dirname, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "            if img.shape != (horiz_x, vert_y, 3):\n",
        "                counter += 1\n",
        "\n",
        "    print('Sanity result: ' + str(counter))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at3uxTKnHvKr"
      },
      "source": [
        "\n",
        "def show_img(index, X, Y):\n",
        "    # plt.imshow(X[index])\n",
        "    plt.imshow(cv2.cvtColor(X[index],cv2.COLOR_BGR2RGB).astype('float32'))\n",
        "    plt.show()\n",
        "    nmb = int(np.where(Y[index] == 1)[0])\n",
        "    print(\"On image: \" + class_names[nmb])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXO5S6_IL1Rp"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKpUsMrA4gkt"
      },
      "source": [
        "\n",
        "def load_dataset():\n",
        "    trainlist = glob.glob(f'{imgs_train_path}/*/*')\n",
        "    testlist = glob.glob(f'{imgs_test_path}/*/*')\n",
        "    X_train = np.array( [np.array(cv2.normalize(cv2.imread(fname), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)) for fname in trainlist] )\n",
        "    X_test = np.array( [np.array(cv2.normalize(cv2.imread(fname), None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)) for fname in testlist] )\n",
        "\n",
        "    # Y_train = np.array([np.zeros(27) for fname in trainlist])\n",
        "    Y_train = np.array([0 for fname in trainlist])\n",
        "    for i,fname in enumerate(trainlist):\n",
        "        # print(fname)\n",
        "        img_id = fname.split('/')[7]##7 if windows 4 if manjaro\n",
        "        img_id = img_id.split('_')[0]\n",
        "        # Y_train[i][img_id] = 1\n",
        "        Y_train[i] = img_id\n",
        "\n",
        "\n",
        "    # Y_test = np.array([np.zeros(27) for fname in testlist])\n",
        "    Y_test = np.array([0 for fname in testlist])\n",
        "    for i,fname in enumerate(testlist):\n",
        "        img_id = fname.split('/')[7]\n",
        "        img_id = img_id.split('_')[0]\n",
        "        # Y_test[i][img_id] = 1\n",
        "        Y_test[i] = img_id\n",
        "\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "# load_dataset()\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52QUbDo5L9OC"
      },
      "source": [
        "## Create network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGL1a8P5W1Ku"
      },
      "source": [
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    # kernel_initializer = initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
        "    kernel_initializer=None \n",
        "    activ_func = 'relu'\n",
        "    kernel_regularizer=None\n",
        "    activity_regularizer=None\n",
        "\n",
        "    # CONVOLUTIONAL LAYER - 1\n",
        "    model.add(Conv2D(\n",
        "        filters=6, \n",
        "        kernel_size=(5,5), \n",
        "        input_shape=(vert_y, horiz_x, 3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "    # CONVOLUTIONAL LAYER - 1\n",
        "    model.add(Conv2D(\n",
        "        filters=6, \n",
        "        kernel_size=(5,5), \n",
        "        input_shape=(vert_y, horiz_x, 3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "\n",
        "    # POOLING LAYER - 1\n",
        "    model.add(MaxPool2D(\n",
        "        pool_size=(2,2),\n",
        "        strides=(2,2)\n",
        "    ))\n",
        "\n",
        "    # CONVOLUTIONAL LAYER - 2\n",
        "    model.add(Conv2D(\n",
        "        filters=16, \n",
        "        kernel_size=(3,3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "    # CONVOLUTIONAL LAYER - 2\n",
        "    model.add(Conv2D(\n",
        "        filters=16, \n",
        "        kernel_size=(3,3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "\n",
        "    # POOLING LAYER - 2\n",
        "    model.add(MaxPool2D(\n",
        "        pool_size=(2,2),\n",
        "        strides=(2,2)\n",
        "    ))\n",
        "\n",
        "    # CONVOLUTIONAL LAYER - 3\n",
        "    model.add(Conv2D(\n",
        "        filters=32, \n",
        "        kernel_size=(3,3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "\n",
        "    # POOLING LAYER - 3\n",
        "    model.add(MaxPool2D(\n",
        "        pool_size=(2,2),\n",
        "        strides=(2,2)\n",
        "    ))\n",
        "\n",
        "    # CONVOLUTIONAL LAYER - 4\n",
        "    model.add(Conv2D(\n",
        "        filters=60, \n",
        "        kernel_size=(3,3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "\n",
        "    # POOLING LAYER - 4\n",
        "    model.add(MaxPool2D(\n",
        "        pool_size=(2,2),\n",
        "        strides=(2,2)\n",
        "    ))\n",
        "\n",
        "    # CONVOLUTIONAL LAYER - 5\n",
        "    model.add(Conv2D(\n",
        "        filters=120, \n",
        "        kernel_size=(3,3), \n",
        "        activation=activ_func,\n",
        "        # kernel_initializer=kernel_initializer\n",
        "        # kernel_regularizer=kernel_regularizer\n",
        "        # activity_regularizer=activity_regularizer\n",
        "    ))\n",
        "\n",
        "    # POOLING LAYER - 5\n",
        "    model.add(MaxPool2D(\n",
        "        pool_size=(2,2),\n",
        "        strides=(2,2)\n",
        "    ))\n",
        "\n",
        "    ######## FLATTEN ########\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    # model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(27, activation='softmax'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer='adam', \n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aR-plpcMYMG"
      },
      "source": [
        "# Loading dataset\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c60j15t047-T"
      },
      "source": [
        "## Resize images and do sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe0BVKLffdt3"
      },
      "source": [
        "# Uncomment before first run on dataset \n",
        "# resize(imgs_train_path)\n",
        "# sanity_check(imgs_train_path)\n",
        "# resize(imgs_test_path)\n",
        "# sanity_check(imgs_test_path)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3YGc40zMT07"
      },
      "source": [
        "# X_train, Y_train, X_test, Y_test = load_dataset()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2_6niPw-o-I"
      },
      "source": [
        "# One hot encoding\n",
        "# Y_cat_train = to_categorical(Y_train, 27)\n",
        "# Y_cat_test = to_categorical(Y_test, 27)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbm9XBKCPtn"
      },
      "source": [
        "## Data augumentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRThxI7Y8qKw"
      },
      "source": [
        "## Initializing ImageDataGenerator class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0PtMORrCPDN"
      },
      "source": [
        "image_gen = ImageDataGenerator(rotation_range=5, # rotate the image 20 degrees\n",
        "                               width_shift_range=0.05, # Shift the pic width by a max of 5%\n",
        "                               height_shift_range=0.05, # Shift the pic height by a max of 5%\n",
        "                            #    rescale=1.1, # Rescale the image by normalzing it.\n",
        "                               shear_range=0.05, # Shear means cutting away part of the image (max 10%)\n",
        "                               zoom_range=0.05, # Zoom in by 10% max\n",
        "                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n",
        "                                )\n",
        "                              "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryU4cvqRuDJL"
      },
      "source": [
        "## Augumentation sample result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2dLQoeLEgOq"
      },
      "source": [
        "#augumentation sample result\n",
        "\n",
        "#original img\n",
        "# nmb = random.randint(0, 296)\n",
        "# my_hand = X_test[nmb]\n",
        "# show_img(nmb, X_test, Y_cat_test)\n",
        "\n",
        "# #generated img\n",
        "# gen_img = image_gen.random_transform(my_hand)\n",
        "# print(\"Generated image\")\n",
        "# plt.imshow(cv2.cvtColor(gen_img,cv2.COLOR_BGR2RGB).astype('float32'))\n",
        "# plt.show()\n",
        "\n",
        "# comparison = my_hand == gen_img\n",
        "# equal_arrays = comparison.all()\n",
        "# print(\"Images are equal?: \" + str(equal_arrays))"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM2dXXIsuIRj"
      },
      "source": [
        "## Image shape and batch size initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHoIHLMFMP_3",
        "outputId": "9e048d6b-49e3-4266-8b1b-7199b626f80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "image_shape = (horiz_x, vert_y, 3)\n",
        "batch_size = 16\n",
        "\n",
        "print(imgs_train_path)\n",
        "print(imgs_test_path)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Rozszerzony_dataset/Train/\n",
            "/content/drive/My Drive/Colab Notebooks/Rozszerzony_dataset/Test/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnjTP4lG82TX"
      },
      "source": [
        "## Initializing test and train image geneartors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re4ZJ1TZTm4o",
        "outputId": "b3bc0f8b-00c6-4874-85c2-e6e37e0550fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_image_gen = image_gen.flow_from_directory(imgs_train_path,\n",
        "                                               target_size=image_shape[:2],\n",
        "                                                color_mode='rgb',\n",
        "                                               batch_size=batch_size,\n",
        "                                               class_mode='categorical')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1561 images belonging to 27 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrWwYpgUTm4r",
        "outputId": "7e4b2e82-edb9-4c53-fab1-c14910435e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_image_gen = image_gen.flow_from_directory(imgs_test_path,\n",
        "                                               target_size=image_shape[:2],\n",
        "                                               color_mode='rgb',\n",
        "                                               batch_size=batch_size,\n",
        "                                               class_mode='categorical',shuffle=False)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 297 images belonging to 27 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdwaoT7yMuME"
      },
      "source": [
        "#train_image_gen.class_indices"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYxO9mzpMfAc"
      },
      "source": [
        "# Create and train model\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPcKT1LCpeaO",
        "outputId": "21faa92b-9957-4096-efcb-746e1593f950",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "CNN = create_model()\n",
        "CNN.summary()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 596, 796, 6)       456       \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 592, 792, 6)       906       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 296, 396, 6)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 294, 394, 16)      880       \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 292, 392, 16)      2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 146, 196, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 144, 194, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 72, 97, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 70, 95, 60)        17340     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 35, 47, 60)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 33, 45, 120)       64920     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 16, 22, 120)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 42240)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               5406848   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 27)                1755      \n",
            "=================================================================\n",
            "Total params: 5,508,321\n",
            "Trainable params: 5,508,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Mj2Adg6WR6"
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
        "#Y_cat_train.shape"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxzkZIjI8-SR"
      },
      "source": [
        "## Fit model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhC9fdDfi5V"
      },
      "source": [
        "# CNN.fit(X_train, Y_cat_train, epochs=12, validation_data=(X_test, Y_cat_test), batch_size=batch_size, callbacks=[early_stop])"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbhE5_p19EkS"
      },
      "source": [
        "## Fit model with data augumentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9OcwzpmM-Fw",
        "outputId": "f4cfe254-6ab4-40d6-95e0-c0eb8f902d5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "results = CNN.fit(train_image_gen, epochs=32, validation_data=test_image_gen, callbacks=[early_stop])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/32\n",
            "98/98 [==============================] - 172s 2s/step - loss: 3.2955 - accuracy: 0.0468 - val_loss: 3.2825 - val_accuracy: 0.0370\n",
            "Epoch 2/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 3.3017 - accuracy: 0.0455 - val_loss: 3.2942 - val_accuracy: 0.0505\n",
            "Epoch 3/32\n",
            "98/98 [==============================] - 170s 2s/step - loss: 3.2765 - accuracy: 0.0570 - val_loss: 3.2712 - val_accuracy: 0.0438\n",
            "Epoch 4/32\n",
            "98/98 [==============================] - 167s 2s/step - loss: 3.2279 - accuracy: 0.0679 - val_loss: 3.1805 - val_accuracy: 0.0875\n",
            "Epoch 5/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 3.1145 - accuracy: 0.0730 - val_loss: 2.9901 - val_accuracy: 0.1145\n",
            "Epoch 6/32\n",
            "98/98 [==============================] - 177s 2s/step - loss: 2.9712 - accuracy: 0.1262 - val_loss: 2.8780 - val_accuracy: 0.1515\n",
            "Epoch 7/32\n",
            "98/98 [==============================] - 172s 2s/step - loss: 2.8546 - accuracy: 0.1461 - val_loss: 2.7942 - val_accuracy: 0.1549\n",
            "Epoch 8/32\n",
            "98/98 [==============================] - 174s 2s/step - loss: 2.7015 - accuracy: 0.1800 - val_loss: 2.6239 - val_accuracy: 0.2020\n",
            "Epoch 9/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 2.5404 - accuracy: 0.2357 - val_loss: 2.4951 - val_accuracy: 0.2525\n",
            "Epoch 10/32\n",
            "98/98 [==============================] - 171s 2s/step - loss: 2.3990 - accuracy: 0.2588 - val_loss: 2.4245 - val_accuracy: 0.2189\n",
            "Epoch 11/32\n",
            "98/98 [==============================] - 167s 2s/step - loss: 2.2923 - accuracy: 0.2787 - val_loss: 2.3579 - val_accuracy: 0.2593\n",
            "Epoch 12/32\n",
            "98/98 [==============================] - 167s 2s/step - loss: 2.1697 - accuracy: 0.2960 - val_loss: 2.1678 - val_accuracy: 0.3165\n",
            "Epoch 13/32\n",
            "98/98 [==============================] - 167s 2s/step - loss: 2.0027 - accuracy: 0.3517 - val_loss: 2.1233 - val_accuracy: 0.3367\n",
            "Epoch 14/32\n",
            "98/98 [==============================] - 170s 2s/step - loss: 1.9136 - accuracy: 0.3799 - val_loss: 1.9719 - val_accuracy: 0.3670\n",
            "Epoch 15/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 1.7948 - accuracy: 0.4074 - val_loss: 1.9085 - val_accuracy: 0.3973\n",
            "Epoch 16/32\n",
            "98/98 [==============================] - 169s 2s/step - loss: 1.6255 - accuracy: 0.4689 - val_loss: 1.9250 - val_accuracy: 0.3636\n",
            "Epoch 17/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 1.5955 - accuracy: 0.4773 - val_loss: 1.7269 - val_accuracy: 0.4579\n",
            "Epoch 18/32\n",
            "98/98 [==============================] - 172s 2s/step - loss: 1.5064 - accuracy: 0.5010 - val_loss: 1.6933 - val_accuracy: 0.4411\n",
            "Epoch 19/32\n",
            "98/98 [==============================] - 169s 2s/step - loss: 1.4345 - accuracy: 0.5304 - val_loss: 1.6088 - val_accuracy: 0.4815\n",
            "Epoch 20/32\n",
            "98/98 [==============================] - 172s 2s/step - loss: 1.3448 - accuracy: 0.5586 - val_loss: 1.7393 - val_accuracy: 0.4613\n",
            "Epoch 21/32\n",
            "98/98 [==============================] - 181s 2s/step - loss: 1.3334 - accuracy: 0.5535 - val_loss: 1.6264 - val_accuracy: 0.4310\n",
            "Epoch 22/32\n",
            "98/98 [==============================] - 174s 2s/step - loss: 1.2224 - accuracy: 0.5926 - val_loss: 1.5243 - val_accuracy: 0.5051\n",
            "Epoch 23/32\n",
            "98/98 [==============================] - 181s 2s/step - loss: 1.2212 - accuracy: 0.6022 - val_loss: 1.6396 - val_accuracy: 0.5051\n",
            "Epoch 24/32\n",
            "98/98 [==============================] - 177s 2s/step - loss: 1.0894 - accuracy: 0.6419 - val_loss: 1.6170 - val_accuracy: 0.4343\n",
            "Epoch 25/32\n",
            "98/98 [==============================] - 174s 2s/step - loss: 1.1058 - accuracy: 0.6445 - val_loss: 1.4895 - val_accuracy: 0.5084\n",
            "Epoch 26/32\n",
            "98/98 [==============================] - 176s 2s/step - loss: 1.0409 - accuracy: 0.6586 - val_loss: 1.6415 - val_accuracy: 0.5118\n",
            "Epoch 27/32\n",
            "98/98 [==============================] - 171s 2s/step - loss: 0.9837 - accuracy: 0.6778 - val_loss: 1.3487 - val_accuracy: 0.5387\n",
            "Epoch 28/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 0.9182 - accuracy: 0.6893 - val_loss: 1.5979 - val_accuracy: 0.5051\n",
            "Epoch 29/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 0.8637 - accuracy: 0.7136 - val_loss: 1.5128 - val_accuracy: 0.5421\n",
            "Epoch 30/32\n",
            "98/98 [==============================] - 168s 2s/step - loss: 0.8959 - accuracy: 0.7117 - val_loss: 1.4976 - val_accuracy: 0.5253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAS0RR_jhsfI"
      },
      "source": [
        "## Saving model to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAnrWeRXhva6"
      },
      "source": [
        "CNN.save('hand_gesture_classifier.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flOnxIQROl4_"
      },
      "source": [
        "# Results\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtaLSZAtgzPK"
      },
      "source": [
        "metrics = pd.DataFrame(CNN.history.history)\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFytDgrDOzFi"
      },
      "source": [
        "## Single image prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOhVHB_8VBdH"
      },
      "source": [
        "# nmb = 2\n",
        "# my_hand = X_test[nmb]\n",
        "# show_img(nmb, X_test, Y_cat_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3AUJxpIW5or"
      },
      "source": [
        "# my_hand = my_hand.reshape(1, vert_y, horiz_x, 3)\n",
        "# result = CNN.predict_classes(my_hand)\n",
        "# # result = (CNN.predict(my_hand) > 0.5).astype(\"int32\")\n",
        "# result = int(result)\n",
        "# print('CNN says: ' + class_names[result])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQsDgI5RO3lm"
      },
      "source": [
        "## Predictions for individual classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1CCpmyR-o_O"
      },
      "source": [
        "predictions = CNN.predict_classes(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv3EpAC2XDKz"
      },
      "source": [
        "print(classification_report(Y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAVH-aNsDY0F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}